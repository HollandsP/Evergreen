name: AI Video Editor CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly regression tests
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'
  POSTGRES_VERSION: '14'
  REDIS_VERSION: '7'
  MIN_COVERAGE: 80
  MAX_PERFORMANCE_REGRESSION: 20  # percent

jobs:
  # ===== Code Quality Checks =====
  lint-and-format:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Run Black formatter check
      run: black --check --diff src/ api/ tests/
    
    - name: Run isort import checker
      run: isort --check-only --diff src/ api/ tests/
    
    - name: Run Flake8 linter
      run: flake8 src/ api/ tests/ --config=.flake8
    
    - name: Run MyPy type checker
      run: mypy src/ api/ --ignore-missing-imports
    
    - name: Run Pylint
      run: pylint src/ api/ --rcfile=.pylintrc || true

  # ===== Security Scanning =====
  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Check Python dependencies
      run: |
        pip install safety
        safety check --file requirements.txt
    
    - name: Check npm dependencies
      working-directory: ./web
      run: |
        npm audit --audit-level=moderate

  # ===== Unit Tests =====
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: [lint-and-format]
    
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    services:
      postgres:
        image: postgres:${{ env.POSTGRES_VERSION }}
        env:
          POSTGRES_USER: pipeline
          POSTGRES_PASSWORD: pipeline
          POSTGRES_DB: evergreen_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:${{ env.REDIS_VERSION }}
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg libavcodec-dev libavformat-dev
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run unit tests with coverage
      env:
        DATABASE_URL: postgresql://pipeline:pipeline@localhost:5432/evergreen_test
        REDIS_URL: redis://localhost:6379
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
    
    - name: Check test coverage
      run: |
        coverage report --fail-under=${{ env.MIN_COVERAGE }}
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # ===== Integration Tests =====
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    
    services:
      postgres:
        image: postgres:${{ env.POSTGRES_VERSION }}
        env:
          POSTGRES_USER: pipeline
          POSTGRES_PASSWORD: pipeline
          POSTGRES_DB: evergreen_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:${{ env.REDIS_VERSION }}
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://pipeline:pipeline@localhost:5432/evergreen_test
        REDIS_URL: redis://localhost:6379
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
        RUNWAY_API_KEY: ${{ secrets.RUNWAY_API_KEY }}
      run: |
        pytest tests/integration/ -v --tb=short
    
    - name: Run GPT-4 command parsing tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        pytest tests/integration/test_command_parsing.py -v -k "test_overall_parsing_accuracy"

  # ===== Performance Tests =====
  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [unit-tests]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Download performance baselines
      uses: actions/cache@v3
      with:
        path: ./benchmark_results/baselines.json
        key: performance-baselines-${{ github.sha }}
        restore-keys: |
          performance-baselines-
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance/benchmark_suite.py -v -s
    
    - name: Check for performance regressions
      run: |
        python -c "
        import json
        import sys
        
        try:
            with open('./benchmark_results/performance_report.json', 'r') as f:
                report = json.load(f)
            
            regressions = []
            for op, data in report.get('operations', {}).items():
                if data.get('regression_status') == 'FAIL':
                    regression_pct = data.get('regression_percent', 0)
                    if abs(regression_pct) > ${{ env.MAX_PERFORMANCE_REGRESSION }}:
                        regressions.append(f'{op}: {regression_pct:.1f}% regression')
            
            if regressions:
                print('Performance regressions detected:')
                for r in regressions:
                    print(f'  - {r}')
                sys.exit(1)
            else:
                print('No significant performance regressions detected')
        except Exception as e:
            print(f'Error checking performance: {e}')
            sys.exit(0)  # Don't fail on benchmark errors
        "
    
    - name: Upload performance results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          ./benchmark_results/performance_report.json
          ./benchmark_results/*.png

  # ===== Stress Tests =====
  stress-tests:
    name: Stress Tests
    runs-on: ubuntu-latest
    needs: [integration-tests]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run concurrent operation stress tests
      run: |
        pytest tests/stress/concurrent_operations.py::TestConcurrentOperations::test_concurrent_video_processing -v -s
    
    - name: Run memory pressure tests
      run: |
        pytest tests/stress/concurrent_operations.py::TestConcurrentOperations::test_memory_pressure_handling -v -s
    
    - name: Check resource usage
      run: |
        echo "System resource usage during stress tests:"
        free -h
        df -h

  # ===== Visual Quality Tests =====
  visual-tests:
    name: Visual Quality Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg libopencv-dev python3-opencv
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install opencv-python scikit-image pillow imagehash
    
    - name: Download visual baselines
      uses: actions/cache@v3
      with:
        path: ./visual_baselines/
        key: visual-baselines-${{ github.sha }}
        restore-keys: |
          visual-baselines-
    
    - name: Run visual regression tests
      run: |
        pytest tests/visual/video_output_regression.py::TestVideoOutputQuality -v -s
    
    - name: Upload visual test results
      if: failure()
      uses: actions/upload-artifact@v3
      with:
        name: visual-test-failures
        path: |
          ./visual_baselines/
          ./tests/visual/failed_comparisons/

  # ===== Frontend Tests =====
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: web/package-lock.json
    
    - name: Install dependencies
      working-directory: ./web
      run: npm ci
    
    - name: Run linting
      working-directory: ./web
      run: npm run lint
    
    - name: Run type checking
      working-directory: ./web
      run: npm run type-check || true
    
    - name: Run frontend tests
      working-directory: ./web
      run: npm test -- --coverage --watchAll=false
    
    - name: Build frontend
      working-directory: ./web
      run: npm run build

  # ===== Docker Build =====
  docker-build:
    name: Docker Build Test
    runs-on: ubuntu-latest
    needs: [lint-and-format, security-scan]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Build Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: false
        tags: evergreen:test
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Test Docker image
      run: |
        docker run --rm evergreen:test python -c "import src; print('Docker image OK')"

  # ===== Deploy to Staging =====
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, frontend-tests, docker-build]
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add actual deployment steps here

  # ===== Deploy to Production =====
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, stress-tests, visual-tests, frontend-tests, docker-build]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Create deployment
      uses: actions/github-script@v6
      with:
        script: |
          const deployment = await github.rest.repos.createDeployment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            ref: context.sha,
            environment: 'production',
            required_contexts: [],
            auto_merge: false
          });
          
          console.log(`Created deployment: ${deployment.data.id}`);
    
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Add actual deployment steps here

  # ===== Quality Gate Summary =====
  quality-gate:
    name: Quality Gate Check
    runs-on: ubuntu-latest
    needs: [lint-and-format, security-scan, unit-tests, integration-tests, performance-tests, stress-tests, visual-tests, frontend-tests]
    if: always()
    
    steps:
    - name: Check quality gate status
      run: |
        echo "Quality Gate Summary:"
        echo "===================="
        
        if [[ "${{ needs.lint-and-format.result }}" != "success" ]]; then
          echo "❌ Code quality checks failed"
          exit 1
        else
          echo "✅ Code quality checks passed"
        fi
        
        if [[ "${{ needs.security-scan.result }}" != "success" ]]; then
          echo "❌ Security scan failed"
          exit 1
        else
          echo "✅ Security scan passed"
        fi
        
        if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then
          echo "❌ Unit tests failed"
          exit 1
        else
          echo "✅ Unit tests passed"
        fi
        
        if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then
          echo "❌ Integration tests failed"
          exit 1
        else
          echo "✅ Integration tests passed"
        fi
        
        if [[ "${{ needs.performance-tests.result }}" != "success" ]]; then
          echo "⚠️  Performance tests failed (non-blocking)"
        else
          echo "✅ Performance tests passed"
        fi
        
        if [[ "${{ needs.stress-tests.result }}" != "success" ]]; then
          echo "⚠️  Stress tests failed (non-blocking)"
        else
          echo "✅ Stress tests passed"
        fi
        
        if [[ "${{ needs.visual-tests.result }}" != "success" ]]; then
          echo "⚠️  Visual tests failed (non-blocking)"
        else
          echo "✅ Visual tests passed"
        fi
        
        if [[ "${{ needs.frontend-tests.result }}" != "success" ]]; then
          echo "❌ Frontend tests failed"
          exit 1
        else
          echo "✅ Frontend tests passed"
        fi
        
        echo ""
        echo "Overall Quality Gate: PASSED ✅"